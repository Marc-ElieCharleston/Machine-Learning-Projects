{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0ce7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\core.py:201: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\ipykernel_launcher.py:139: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Episode : 0 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 32\n",
      "Average of last 100 steps : 0.32\n",
      "Average of all steps : 32.0\n",
      "Epsilon : 0.8448244495823493\n",
      "Frames total : 32\n",
      "00:00:00\n",
      "\n",
      "*** Episode : 10 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 230\n",
      "Average of last 100 steps : 2.62\n",
      "Average of all steps : 23.818181818181817\n",
      "Epsilon : 0.5370110218906584\n",
      "Frames total : 262\n",
      "00:00:00\n",
      "\n",
      "*** Episode : 20 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 304\n",
      "Average of last 100 steps : 5.66\n",
      "Average of all steps : 26.952380952380953\n",
      "Epsilon : 0.29692517363542875\n",
      "Frames total : 566\n",
      "00:00:00\n",
      "\n",
      "*** Episode : 30 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 454\n",
      "Average of last 100 steps : 10.2\n",
      "Average of all steps : 32.903225806451616\n",
      "Epsilon : 0.12572555268179905\n",
      "Frames total : 1020\n",
      "00:00:01\n",
      "\n",
      "*** Episode : 40 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 305\n",
      "Average of last 100 steps : 13.25\n",
      "Average of all steps : 32.31707317073171\n",
      "Epsilon : 0.07287957962378233\n",
      "Frames total : 1325\n",
      "00:00:02\n",
      "\n",
      "*** Episode : 50 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 363\n",
      "Average of last 100 steps : 16.88\n",
      "Average of all steps : 33.09803921568628\n",
      "Epsilon : 0.04042368639407362\n",
      "Frames total : 1688\n",
      "00:00:02\n",
      "\n",
      "*** Episode : 60 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 722\n",
      "Average of last 100 steps : 24.1\n",
      "Average of all steps : 39.50819672131148\n",
      "Epsilon : 0.017179440553798657\n",
      "Frames total : 2410\n",
      "00:00:03\n",
      "\n",
      "*** Episode : 70 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 1404\n",
      "Average of last 100 steps : 38.14\n",
      "Average of all steps : 53.71830985915493\n",
      "Epsilon : 0.010433103505303727\n",
      "Frames total : 3814\n",
      "00:00:06\n",
      "\n",
      "*** Episode : 80 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 4213\n",
      "Average of last 100 steps : 80.27\n",
      "Average of all steps : 99.09876543209876\n",
      "Epsilon : 0.010000094891299499\n",
      "Frames total : 8027\n",
      "00:00:13\n",
      "\n",
      "*** Episode : 90 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 3212\n",
      "Average of last 100 steps : 112.39\n",
      "Average of all steps : 123.50549450549451\n",
      "Epsilon : 0.01000000015392836\n",
      "Frames total : 11239\n",
      "00:00:18\n",
      "\n",
      "*** Episode : 100 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 3031\n",
      "Average of last 100 steps : 142.38\n",
      "Average of all steps : 141.2871287128713\n",
      "Epsilon : 0.010000000000358613\n",
      "Frames total : 14270\n",
      "00:00:24\n",
      "\n",
      "*** Episode : 110 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 3816\n",
      "Average of last 100 steps : 178.24\n",
      "Average of all steps : 162.93693693693695\n",
      "Epsilon : 0.010000000000000174\n",
      "Frames total : 18086\n",
      "00:00:30\n",
      "\n",
      "Solved after : 115\n",
      "*** Episode : 120 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 3827\n",
      "Average of last 100 steps : 213.47\n",
      "Average of all steps : 181.099173553719\n",
      "Epsilon : 0.01\n",
      "Frames total : 21913\n",
      "00:00:37\n",
      "\n",
      "*** Episode : 130 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 3941\n",
      "Average of last 100 steps : 248.34\n",
      "Average of all steps : 197.3587786259542\n",
      "Epsilon : 0.01\n",
      "Frames total : 25854\n",
      "00:00:43\n",
      "\n",
      "*** Episode : 140 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 4201\n",
      "Average of last 100 steps : 287.3\n",
      "Average of all steps : 213.15602836879432\n",
      "Epsilon : 0.01\n",
      "Frames total : 30055\n",
      "00:00:50\n",
      "\n",
      "*** Episode : 150 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 3644\n",
      "Average of last 100 steps : 320.11\n",
      "Average of all steps : 223.17218543046357\n",
      "Epsilon : 0.01\n",
      "Frames total : 33699\n",
      "00:00:56\n",
      "\n",
      "*** Episode : 160 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 4457\n",
      "Average of last 100 steps : 357.46\n",
      "Average of all steps : 236.9937888198758\n",
      "Epsilon : 0.01\n",
      "Frames total : 38156\n",
      "00:01:04\n",
      "\n",
      "*** Episode : 170 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 4845\n",
      "Average of last 100 steps : 391.87\n",
      "Average of all steps : 251.46783625730995\n",
      "Epsilon : 0.01\n",
      "Frames total : 43001\n",
      "00:01:11\n",
      "\n",
      "*** Episode : 180 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 4528\n",
      "Average of last 100 steps : 395.02\n",
      "Average of all steps : 262.5911602209945\n",
      "Epsilon : 0.01\n",
      "Frames total : 47529\n",
      "00:01:18\n",
      "\n",
      "*** Episode : 190 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 4673\n",
      "Average of last 100 steps : 409.63\n",
      "Average of all steps : 273.3089005235602\n",
      "Epsilon : 0.01\n",
      "Frames total : 52202\n",
      "00:01:25\n",
      "\n",
      "*** Episode : 200 ***\n",
      "Average reward : \n",
      "Sum total last 10 steps : 4831\n",
      "Average of last 100 steps : 427.63\n",
      "Average of all steps : 283.74626865671644\n",
      "Epsilon : 0.01\n",
      "Frames total : 57033\n",
      "00:01:33\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5936\\2149262624.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mqnet_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5936\\2149262624.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[0mpredicted_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "gamma = 0.9999\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_memory_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "update_target_frequency = 500\n",
    "\n",
    "egreedy = 0.9 # pourcentage exploration/exploitation\n",
    "egreedy_final = 0.01 # finir l'algo avec ce pourcentage exploration/exploitation\n",
    "egreedy_decay = 500 #modification du eggreey par episode\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay)\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "\n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "       return zip(*random.sample(self.memory, batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # super(NeuralNetwork, self).__init__()\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "\n",
    "        return output2\n",
    "\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        # self.loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.update_target_counter = 0\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "\n",
    "        if random_for_egreedy > epsilon:\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def optimize(self):\n",
    "\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "\n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "\n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]\n",
    "\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1,max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "\n",
    "        target_value = reward + (1-done) * gamma * max_new_state_values\n",
    "\n",
    "\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "\n",
    "        self.update_target_counter\n",
    "\n",
    "\n",
    "        # Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "\n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    step = 0\n",
    "    # for step in range(100):\n",
    "    while True:\n",
    "\n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "\n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "\n",
    "        # action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "\n",
    "        state = new_state\n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "\n",
    "            if mean_reward_100 > score_to_solve and solved == False:\n",
    "                print(\"Solved after : \"+str(i_episode))\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "\n",
    "            if i_episode % report_interval == 0:\n",
    "                print(\"*** Episode : \" + str(i_episode)+\" ***\")\n",
    "                print(\"Average reward : \" + str())\n",
    "                print(\"Sum total last 10 steps : \" +str(sum(steps_total[-report_interval:])))\n",
    "                print(\"Average of last 100 steps : \" + str(mean_reward_100))\n",
    "                print(\"Average of all steps : \" + str(sum(steps_total)/len(steps_total)))\n",
    "                print(\"Epsilon : \" +str(epsilon))\n",
    "                print(\"Frames total : \" + str(frames_total))\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "                print()\n",
    "\n",
    "            break\n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total) / num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:]) / 100))\n",
    "\n",
    "if solved :\n",
    "    print(\"Solved after : \" + str(solved_after))\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green', width=5)\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eaf97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
