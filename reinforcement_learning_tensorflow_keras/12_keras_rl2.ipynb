{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888d6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f9dbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0994b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdef8bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\envs\\registration.py:565: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
     ]
    }
   ],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e852b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor step in range(200):\\n    env.render()\\n    action = env.action_space.sample()\\n    env.step(action)\\n    \\nenv.close()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "\"\"\"\n",
    "for step in range(200):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)\n",
    "    \n",
    "env.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe68a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a76e6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077ee66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1,)+nb_obs))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bd8dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f119854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit = 20000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39db4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aab2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                             attr = 'eps',\n",
    "                             value_max = 1.0,\n",
    "                             value_min = 0.1,\n",
    "                             value_test = 0.05,\n",
    "                             nb_steps = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a4b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions,\n",
    "               memory=memory,\n",
    "               nb_steps_warmup=10,\n",
    "               target_model_update =100,\n",
    "               policy=policy)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f48abd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=1e-3), metrics =['mae'])\n",
    "dqn.compile(Adam(learning_rate = 1e-3), metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63608c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\pc\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    34/20000: episode: 1, duration: 0.457s, episode steps:  34, steps per second:  74, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.504182, mae: 0.549416, mean_q: 0.270289, mean_eps: 0.999010\n",
      "    50/20000: episode: 2, duration: 0.071s, episode steps:  16, steps per second: 227, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.369391, mae: 0.564133, mean_q: 0.491381, mean_eps: 0.998132\n",
      "    59/20000: episode: 3, duration: 0.041s, episode steps:   9, steps per second: 219, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.261250, mae: 0.621697, mean_q: 0.707608, mean_eps: 0.997570\n",
      "    78/20000: episode: 4, duration: 0.082s, episode steps:  19, steps per second: 231, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.147940, mae: 0.663164, mean_q: 0.895780, mean_eps: 0.996940\n",
      "   119/20000: episode: 5, duration: 0.170s, episode steps:  41, steps per second: 242, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.390 [0.000, 1.000],  loss: 0.238475, mae: 0.826301, mean_q: 1.114066, mean_eps: 0.995590\n",
      "   139/20000: episode: 6, duration: 0.084s, episode steps:  20, steps per second: 239, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.143969, mae: 1.125644, mean_q: 1.966565, mean_eps: 0.994218\n",
      "   154/20000: episode: 7, duration: 0.064s, episode steps:  15, steps per second: 236, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.092080, mae: 1.116826, mean_q: 2.047834, mean_eps: 0.993430\n",
      "   173/20000: episode: 8, duration: 0.083s, episode steps:  19, steps per second: 230, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.263 [0.000, 1.000],  loss: 0.064739, mae: 1.064391, mean_q: 1.960717, mean_eps: 0.992665\n",
      "   205/20000: episode: 9, duration: 0.134s, episode steps:  32, steps per second: 240, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.113358, mae: 1.110490, mean_q: 1.986771, mean_eps: 0.991518\n",
      "   221/20000: episode: 10, duration: 0.069s, episode steps:  16, steps per second: 233, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.320389, mae: 1.561480, mean_q: 2.557401, mean_eps: 0.990438\n",
      "   252/20000: episode: 11, duration: 0.129s, episode steps:  31, steps per second: 240, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.152332, mae: 1.560540, mean_q: 2.932477, mean_eps: 0.989380\n",
      "   292/20000: episode: 12, duration: 0.165s, episode steps:  40, steps per second: 243, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.125211, mae: 1.524618, mean_q: 2.864351, mean_eps: 0.987783\n",
      "   316/20000: episode: 13, duration: 0.107s, episode steps:  24, steps per second: 225, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.287948, mae: 1.796680, mean_q: 3.084468, mean_eps: 0.986343\n",
      "   330/20000: episode: 14, duration: 0.060s, episode steps:  14, steps per second: 234, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.241439, mae: 2.025982, mean_q: 3.883248, mean_eps: 0.985487\n",
      "   369/20000: episode: 15, duration: 0.174s, episode steps:  39, steps per second: 224, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 0.146459, mae: 1.976846, mean_q: 3.755616, mean_eps: 0.984295\n",
      "   383/20000: episode: 16, duration: 0.065s, episode steps:  14, steps per second: 214, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.253551, mae: 2.025149, mean_q: 3.821149, mean_eps: 0.983103\n",
      "   406/20000: episode: 17, duration: 0.104s, episode steps:  23, steps per second: 222, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.251537, mae: 2.059800, mean_q: 3.739742, mean_eps: 0.982270\n",
      "   443/20000: episode: 18, duration: 0.163s, episode steps:  37, steps per second: 228, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 0.235234, mae: 2.432594, mean_q: 4.612719, mean_eps: 0.980920\n",
      "   458/20000: episode: 19, duration: 0.066s, episode steps:  15, steps per second: 229, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.244823, mae: 2.434651, mean_q: 4.656100, mean_eps: 0.979750\n",
      "   471/20000: episode: 20, duration: 0.058s, episode steps:  13, steps per second: 224, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.180114, mae: 2.411123, mean_q: 4.644831, mean_eps: 0.979120\n",
      "   488/20000: episode: 21, duration: 0.073s, episode steps:  17, steps per second: 231, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.191044, mae: 2.395018, mean_q: 4.581702, mean_eps: 0.978445\n",
      "   525/20000: episode: 22, duration: 0.157s, episode steps:  37, steps per second: 235, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.336920, mae: 2.685088, mean_q: 5.018405, mean_eps: 0.977230\n",
      "   538/20000: episode: 23, duration: 0.057s, episode steps:  13, steps per second: 229, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.340336, mae: 2.836176, mean_q: 5.393441, mean_eps: 0.976105\n",
      "   553/20000: episode: 24, duration: 0.064s, episode steps:  15, steps per second: 234, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.308857, mae: 2.810200, mean_q: 5.307063, mean_eps: 0.975475\n",
      "   610/20000: episode: 25, duration: 0.237s, episode steps:  57, steps per second: 240, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 0.327335, mae: 2.883836, mean_q: 5.430609, mean_eps: 0.973855\n",
      "   653/20000: episode: 26, duration: 0.177s, episode steps:  43, steps per second: 243, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 0.305551, mae: 3.244392, mean_q: 6.282145, mean_eps: 0.971605\n",
      "   664/20000: episode: 27, duration: 0.049s, episode steps:  11, steps per second: 225, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.398633, mae: 3.190851, mean_q: 6.165846, mean_eps: 0.970390\n",
      "   674/20000: episode: 28, duration: 0.044s, episode steps:  10, steps per second: 226, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.198863, mae: 3.187062, mean_q: 6.228652, mean_eps: 0.969917\n",
      "   695/20000: episode: 29, duration: 0.088s, episode steps:  21, steps per second: 238, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.338066, mae: 3.240817, mean_q: 6.311027, mean_eps: 0.969220\n",
      "   708/20000: episode: 30, duration: 0.058s, episode steps:  13, steps per second: 225, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.616675, mae: 3.428407, mean_q: 6.300181, mean_eps: 0.968455\n",
      "   728/20000: episode: 31, duration: 0.086s, episode steps:  20, steps per second: 231, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.535541, mae: 3.666863, mean_q: 7.112292, mean_eps: 0.967712\n",
      "   756/20000: episode: 32, duration: 0.117s, episode steps:  28, steps per second: 239, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 0.386514, mae: 3.613917, mean_q: 7.072700, mean_eps: 0.966632\n",
      "   778/20000: episode: 33, duration: 0.095s, episode steps:  22, steps per second: 232, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.397813, mae: 3.630426, mean_q: 7.082875, mean_eps: 0.965508\n",
      "   817/20000: episode: 34, duration: 0.163s, episode steps:  39, steps per second: 239, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.543661, mae: 3.792500, mean_q: 7.284407, mean_eps: 0.964135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   859/20000: episode: 35, duration: 0.173s, episode steps:  42, steps per second: 242, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 0.481380, mae: 4.067354, mean_q: 7.966420, mean_eps: 0.962313\n",
      "   877/20000: episode: 36, duration: 0.076s, episode steps:  18, steps per second: 236, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.505175, mae: 4.073806, mean_q: 7.976498, mean_eps: 0.960963\n",
      "   907/20000: episode: 37, duration: 0.126s, episode steps:  30, steps per second: 239, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.584042, mae: 4.082825, mean_q: 7.905220, mean_eps: 0.959883\n",
      "   927/20000: episode: 38, duration: 0.086s, episode steps:  20, steps per second: 234, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.559395, mae: 4.542288, mean_q: 8.923852, mean_eps: 0.958758\n",
      "   991/20000: episode: 39, duration: 0.263s, episode steps:  64, steps per second: 244, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 0.714436, mae: 4.484943, mean_q: 8.695959, mean_eps: 0.956867\n",
      "  1008/20000: episode: 40, duration: 0.077s, episode steps:  17, steps per second: 221, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.532534, mae: 4.582848, mean_q: 8.790448, mean_eps: 0.955045\n",
      "  1032/20000: episode: 41, duration: 0.101s, episode steps:  24, steps per second: 238, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.690072, mae: 4.913891, mean_q: 9.639871, mean_eps: 0.954122\n",
      "  1056/20000: episode: 42, duration: 0.102s, episode steps:  24, steps per second: 235, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.634078, mae: 4.842054, mean_q: 9.577276, mean_eps: 0.953043\n",
      "  1070/20000: episode: 43, duration: 0.060s, episode steps:  14, steps per second: 233, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.678451, mae: 4.796923, mean_q: 9.470698, mean_eps: 0.952188\n",
      "  1087/20000: episode: 44, duration: 0.072s, episode steps:  17, steps per second: 235, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.852530, mae: 4.878059, mean_q: 9.573186, mean_eps: 0.951490\n",
      "  1115/20000: episode: 45, duration: 0.120s, episode steps:  28, steps per second: 234, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.734508, mae: 5.081895, mean_q: 9.947345, mean_eps: 0.950478\n",
      "  1147/20000: episode: 46, duration: 0.133s, episode steps:  32, steps per second: 241, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.974729, mae: 5.407257, mean_q: 10.685102, mean_eps: 0.949127\n",
      "  1165/20000: episode: 47, duration: 0.076s, episode steps:  18, steps per second: 236, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.967357, mae: 5.325342, mean_q: 10.466473, mean_eps: 0.948002\n",
      "  1181/20000: episode: 48, duration: 0.070s, episode steps:  16, steps per second: 229, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.827509, mae: 5.298674, mean_q: 10.438848, mean_eps: 0.947237\n",
      "  1197/20000: episode: 49, duration: 0.068s, episode steps:  16, steps per second: 234, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 1.071720, mae: 5.434225, mean_q: 10.573654, mean_eps: 0.946518\n",
      "  1250/20000: episode: 50, duration: 0.223s, episode steps:  53, steps per second: 238, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.901035, mae: 5.841154, mean_q: 11.507688, mean_eps: 0.944965\n",
      "  1263/20000: episode: 51, duration: 0.057s, episode steps:  13, steps per second: 228, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.032890, mae: 5.822969, mean_q: 11.430777, mean_eps: 0.943480\n",
      "  1273/20000: episode: 52, duration: 0.044s, episode steps:  10, steps per second: 227, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.271565, mae: 5.887903, mean_q: 11.547802, mean_eps: 0.942962\n",
      "  1284/20000: episode: 53, duration: 0.048s, episode steps:  11, steps per second: 229, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.211234, mae: 5.810550, mean_q: 11.311771, mean_eps: 0.942490\n",
      "  1301/20000: episode: 54, duration: 0.076s, episode steps:  17, steps per second: 223, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.997531, mae: 5.874647, mean_q: 11.585302, mean_eps: 0.941860\n",
      "  1318/20000: episode: 55, duration: 0.073s, episode steps:  17, steps per second: 233, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.244791, mae: 6.297150, mean_q: 12.387017, mean_eps: 0.941095\n",
      "  1337/20000: episode: 56, duration: 0.081s, episode steps:  19, steps per second: 236, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.005509, mae: 6.284190, mean_q: 12.496289, mean_eps: 0.940285\n",
      "  1368/20000: episode: 57, duration: 0.131s, episode steps:  31, steps per second: 237, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 1.216287, mae: 6.292021, mean_q: 12.445765, mean_eps: 0.939160\n",
      "  1404/20000: episode: 58, duration: 0.150s, episode steps:  36, steps per second: 240, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.242420, mae: 6.340763, mean_q: 12.548083, mean_eps: 0.937652\n",
      "  1435/20000: episode: 59, duration: 0.130s, episode steps:  31, steps per second: 238, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 1.298439, mae: 6.986218, mean_q: 13.906089, mean_eps: 0.936145\n",
      "  1447/20000: episode: 60, duration: 0.053s, episode steps:  12, steps per second: 228, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.866499, mae: 6.941608, mean_q: 13.948561, mean_eps: 0.935177\n",
      "  1470/20000: episode: 61, duration: 0.097s, episode steps:  23, steps per second: 236, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 1.585231, mae: 6.956030, mean_q: 13.759672, mean_eps: 0.934390\n",
      "  1482/20000: episode: 62, duration: 0.054s, episode steps:  12, steps per second: 224, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.200862, mae: 6.991193, mean_q: 13.761292, mean_eps: 0.933603\n",
      "  1498/20000: episode: 63, duration: 0.071s, episode steps:  16, steps per second: 225, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.450684, mae: 7.020227, mean_q: 13.913264, mean_eps: 0.932972\n",
      "  1575/20000: episode: 64, duration: 0.317s, episode steps:  77, steps per second: 243, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.254648, mae: 7.220801, mean_q: 14.398118, mean_eps: 0.930880\n",
      "  1604/20000: episode: 65, duration: 0.122s, episode steps:  29, steps per second: 237, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.422371, mae: 7.295383, mean_q: 14.524889, mean_eps: 0.928495\n",
      "  1659/20000: episode: 66, duration: 0.226s, episode steps:  55, steps per second: 243, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 1.645545, mae: 7.744840, mean_q: 15.424590, mean_eps: 0.926605\n",
      "  1685/20000: episode: 67, duration: 0.108s, episode steps:  26, steps per second: 240, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 2.235754, mae: 7.746016, mean_q: 15.287340, mean_eps: 0.924782\n",
      "  1696/20000: episode: 68, duration: 0.050s, episode steps:  11, steps per second: 221, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.359686, mae: 7.879536, mean_q: 15.719498, mean_eps: 0.923950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1723/20000: episode: 69, duration: 0.123s, episode steps:  27, steps per second: 219, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.310583, mae: 8.064503, mean_q: 15.780992, mean_eps: 0.923095\n",
      "  1740/20000: episode: 70, duration: 0.072s, episode steps:  17, steps per second: 235, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.689221, mae: 8.095644, mean_q: 16.157471, mean_eps: 0.922105\n",
      "  1774/20000: episode: 71, duration: 0.141s, episode steps:  34, steps per second: 241, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 1.626761, mae: 8.202408, mean_q: 16.274525, mean_eps: 0.920957\n",
      "  1809/20000: episode: 72, duration: 0.147s, episode steps:  35, steps per second: 238, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.488862, mae: 8.252704, mean_q: 16.368025, mean_eps: 0.919405\n",
      "  1840/20000: episode: 73, duration: 0.128s, episode steps:  31, steps per second: 242, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 1.763337, mae: 8.602979, mean_q: 17.181101, mean_eps: 0.917920\n",
      "  1858/20000: episode: 74, duration: 0.076s, episode steps:  18, steps per second: 237, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.509790, mae: 8.607552, mean_q: 17.324728, mean_eps: 0.916817\n",
      "  1870/20000: episode: 75, duration: 0.055s, episode steps:  12, steps per second: 218, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.072409, mae: 8.509629, mean_q: 17.119228, mean_eps: 0.916142\n",
      "  1882/20000: episode: 76, duration: 0.053s, episode steps:  12, steps per second: 228, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.113666, mae: 8.593510, mean_q: 17.253461, mean_eps: 0.915602\n",
      "  1901/20000: episode: 77, duration: 0.082s, episode steps:  19, steps per second: 231, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.205094, mae: 8.572302, mean_q: 17.254024, mean_eps: 0.914905\n",
      "  1914/20000: episode: 78, duration: 0.056s, episode steps:  13, steps per second: 233, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.919696, mae: 8.893285, mean_q: 17.649956, mean_eps: 0.914185\n",
      "  1931/20000: episode: 79, duration: 0.074s, episode steps:  17, steps per second: 229, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 2.851422, mae: 9.037109, mean_q: 17.948463, mean_eps: 0.913510\n",
      "  1954/20000: episode: 80, duration: 0.099s, episode steps:  23, steps per second: 231, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.402638, mae: 9.054750, mean_q: 18.023761, mean_eps: 0.912610\n",
      "  1985/20000: episode: 81, duration: 0.131s, episode steps:  31, steps per second: 237, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 2.692011, mae: 9.031783, mean_q: 17.889112, mean_eps: 0.911395\n",
      "  2001/20000: episode: 82, duration: 0.071s, episode steps:  16, steps per second: 225, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.451040, mae: 8.909944, mean_q: 17.740511, mean_eps: 0.910338\n",
      "  2047/20000: episode: 83, duration: 0.190s, episode steps:  46, steps per second: 242, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.803372, mae: 9.538424, mean_q: 19.077097, mean_eps: 0.908942\n",
      "  2060/20000: episode: 84, duration: 0.057s, episode steps:  13, steps per second: 230, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 2.305898, mae: 9.488347, mean_q: 18.986051, mean_eps: 0.907615\n",
      "  2073/20000: episode: 85, duration: 0.056s, episode steps:  13, steps per second: 230, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 2.001058, mae: 9.539237, mean_q: 18.952371, mean_eps: 0.907030\n",
      "  2103/20000: episode: 86, duration: 0.129s, episode steps:  30, steps per second: 232, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.198301, mae: 9.596472, mean_q: 19.180662, mean_eps: 0.906063\n",
      "  2131/20000: episode: 87, duration: 0.117s, episode steps:  28, steps per second: 239, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.091584, mae: 9.774892, mean_q: 19.564433, mean_eps: 0.904758\n",
      "  2166/20000: episode: 88, duration: 0.146s, episode steps:  35, steps per second: 240, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.827646, mae: 9.768185, mean_q: 19.646885, mean_eps: 0.903340\n",
      "  2196/20000: episode: 89, duration: 0.126s, episode steps:  30, steps per second: 238, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.139454, mae: 9.845851, mean_q: 19.718180, mean_eps: 0.901877\n",
      "  2213/20000: episode: 90, duration: 0.081s, episode steps:  17, steps per second: 210, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.925828, mae: 9.972744, mean_q: 19.724512, mean_eps: 0.900820\n",
      "  2240/20000: episode: 91, duration: 0.119s, episode steps:  27, steps per second: 227, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.195646, mae: 9.996864, mean_q: 20.069831, mean_eps: 0.899830\n",
      "  2251/20000: episode: 92, duration: 0.049s, episode steps:  11, steps per second: 223, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.395128, mae: 10.108685, mean_q: 20.381458, mean_eps: 0.898975\n",
      "  2273/20000: episode: 93, duration: 0.094s, episode steps:  22, steps per second: 233, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.150467, mae: 10.079944, mean_q: 20.284567, mean_eps: 0.898232\n",
      "  2307/20000: episode: 94, duration: 0.144s, episode steps:  34, steps per second: 236, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.324281, mae: 10.169015, mean_q: 20.357375, mean_eps: 0.896972\n",
      "  2325/20000: episode: 95, duration: 0.078s, episode steps:  18, steps per second: 232, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.746988, mae: 10.639024, mean_q: 21.496163, mean_eps: 0.895802\n",
      "  2342/20000: episode: 96, duration: 0.072s, episode steps:  17, steps per second: 235, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 2.948079, mae: 10.486684, mean_q: 21.042959, mean_eps: 0.895015\n",
      "  2356/20000: episode: 97, duration: 0.061s, episode steps:  14, steps per second: 231, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.337685, mae: 10.679372, mean_q: 21.458303, mean_eps: 0.894317\n",
      "  2374/20000: episode: 98, duration: 0.077s, episode steps:  18, steps per second: 232, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.104151, mae: 10.630455, mean_q: 21.560365, mean_eps: 0.893598\n",
      "  2410/20000: episode: 99, duration: 0.158s, episode steps:  36, steps per second: 228, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.959770, mae: 10.676792, mean_q: 21.482055, mean_eps: 0.892382\n",
      "  2434/20000: episode: 100, duration: 0.118s, episode steps:  24, steps per second: 203, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.380364, mae: 11.158397, mean_q: 22.475356, mean_eps: 0.891033\n",
      "  2464/20000: episode: 101, duration: 0.133s, episode steps:  30, steps per second: 226, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.365286, mae: 11.071059, mean_q: 22.179014, mean_eps: 0.889817\n",
      "  2509/20000: episode: 102, duration: 0.187s, episode steps:  45, steps per second: 240, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.949812, mae: 11.033020, mean_q: 22.135575, mean_eps: 0.888130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2529/20000: episode: 103, duration: 0.086s, episode steps:  20, steps per second: 233, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.350360, mae: 11.369696, mean_q: 22.993651, mean_eps: 0.886667\n",
      "  2575/20000: episode: 104, duration: 0.189s, episode steps:  46, steps per second: 244, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.220200, mae: 11.370589, mean_q: 23.059792, mean_eps: 0.885182\n",
      "  2590/20000: episode: 105, duration: 0.066s, episode steps:  15, steps per second: 227, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.421023, mae: 11.315303, mean_q: 23.021416, mean_eps: 0.883810\n",
      "  2624/20000: episode: 106, duration: 0.143s, episode steps:  34, steps per second: 238, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 2.758898, mae: 11.644643, mean_q: 23.635907, mean_eps: 0.882708\n",
      "  2641/20000: episode: 107, duration: 0.075s, episode steps:  17, steps per second: 227, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 3.732611, mae: 12.043676, mean_q: 24.118720, mean_eps: 0.881560\n",
      "  2656/20000: episode: 108, duration: 0.068s, episode steps:  15, steps per second: 221, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 3.019716, mae: 12.009196, mean_q: 24.199124, mean_eps: 0.880840\n",
      "  2679/20000: episode: 109, duration: 0.098s, episode steps:  23, steps per second: 235, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.622040, mae: 11.886490, mean_q: 24.129469, mean_eps: 0.879985\n",
      "  2696/20000: episode: 110, duration: 0.074s, episode steps:  17, steps per second: 230, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.271865, mae: 11.993888, mean_q: 24.038459, mean_eps: 0.879085\n",
      "  2709/20000: episode: 111, duration: 0.058s, episode steps:  13, steps per second: 223, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 2.369051, mae: 11.945713, mean_q: 24.223794, mean_eps: 0.878410\n",
      "  2720/20000: episode: 112, duration: 0.048s, episode steps:  11, steps per second: 228, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.919510, mae: 12.712085, mean_q: 25.823582, mean_eps: 0.877870\n",
      "  2743/20000: episode: 113, duration: 0.097s, episode steps:  23, steps per second: 236, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 2.469136, mae: 12.381892, mean_q: 25.067411, mean_eps: 0.877105\n",
      "  2755/20000: episode: 114, duration: 0.055s, episode steps:  12, steps per second: 219, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.399301, mae: 12.374933, mean_q: 24.964203, mean_eps: 0.876317\n",
      "  2788/20000: episode: 115, duration: 0.137s, episode steps:  33, steps per second: 241, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.881591, mae: 12.424373, mean_q: 25.151449, mean_eps: 0.875305\n",
      "  2843/20000: episode: 116, duration: 0.230s, episode steps:  55, steps per second: 239, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.418 [0.000, 1.000],  loss: 3.026862, mae: 12.850666, mean_q: 25.987484, mean_eps: 0.873325\n",
      "  2891/20000: episode: 117, duration: 0.199s, episode steps:  48, steps per second: 241, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.467779, mae: 12.877248, mean_q: 26.087441, mean_eps: 0.871008\n",
      "  2909/20000: episode: 118, duration: 0.079s, episode steps:  18, steps per second: 227, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 2.497174, mae: 13.082698, mean_q: 26.397628, mean_eps: 0.869522\n",
      "  2928/20000: episode: 119, duration: 0.082s, episode steps:  19, steps per second: 232, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 2.208202, mae: 13.156328, mean_q: 26.815235, mean_eps: 0.868690\n",
      "  2965/20000: episode: 120, duration: 0.155s, episode steps:  37, steps per second: 239, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.403831, mae: 13.128935, mean_q: 26.629107, mean_eps: 0.867430\n",
      "  2981/20000: episode: 121, duration: 0.069s, episode steps:  16, steps per second: 233, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 1.846215, mae: 13.282881, mean_q: 27.036696, mean_eps: 0.866237\n",
      "  3036/20000: episode: 122, duration: 0.229s, episode steps:  55, steps per second: 240, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.653834, mae: 13.495505, mean_q: 27.483874, mean_eps: 0.864640\n",
      "  3053/20000: episode: 123, duration: 0.073s, episode steps:  17, steps per second: 233, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 3.059892, mae: 13.558197, mean_q: 27.665238, mean_eps: 0.863020\n",
      "  3073/20000: episode: 124, duration: 0.086s, episode steps:  20, steps per second: 232, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.824508, mae: 13.571816, mean_q: 27.719994, mean_eps: 0.862188\n",
      "  3098/20000: episode: 125, duration: 0.108s, episode steps:  25, steps per second: 231, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.578068, mae: 13.578733, mean_q: 27.373895, mean_eps: 0.861175\n",
      "  3122/20000: episode: 126, duration: 0.103s, episode steps:  24, steps per second: 232, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.557856, mae: 14.194767, mean_q: 28.746639, mean_eps: 0.860073\n",
      "  3135/20000: episode: 127, duration: 0.056s, episode steps:  13, steps per second: 231, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.101279, mae: 14.357582, mean_q: 29.164744, mean_eps: 0.859240\n",
      "  3157/20000: episode: 128, duration: 0.096s, episode steps:  22, steps per second: 229, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.759567, mae: 14.414905, mean_q: 29.162969, mean_eps: 0.858453\n",
      "  3177/20000: episode: 129, duration: 0.085s, episode steps:  20, steps per second: 237, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.672454, mae: 14.156518, mean_q: 28.763906, mean_eps: 0.857507\n",
      "  3239/20000: episode: 130, duration: 0.256s, episode steps:  62, steps per second: 242, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.796886, mae: 14.519572, mean_q: 29.477087, mean_eps: 0.855662\n",
      "  3287/20000: episode: 131, duration: 0.197s, episode steps:  48, steps per second: 244, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.675433, mae: 14.889175, mean_q: 30.361529, mean_eps: 0.853187\n",
      "  3307/20000: episode: 132, duration: 0.088s, episode steps:  20, steps per second: 227, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 4.069636, mae: 14.608679, mean_q: 29.540960, mean_eps: 0.851657\n",
      "  3347/20000: episode: 133, duration: 0.167s, episode steps:  40, steps per second: 240, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.454765, mae: 15.019532, mean_q: 30.780920, mean_eps: 0.850308\n",
      "  3365/20000: episode: 134, duration: 0.079s, episode steps:  18, steps per second: 227, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.782944, mae: 14.929324, mean_q: 30.597166, mean_eps: 0.849002\n",
      "  3384/20000: episode: 135, duration: 0.081s, episode steps:  19, steps per second: 236, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 3.768094, mae: 15.105171, mean_q: 30.572658, mean_eps: 0.848170\n",
      "  3412/20000: episode: 136, duration: 0.119s, episode steps:  28, steps per second: 236, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 4.369299, mae: 15.171502, mean_q: 30.733981, mean_eps: 0.847113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3452/20000: episode: 137, duration: 0.166s, episode steps:  40, steps per second: 240, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 2.824111, mae: 15.634170, mean_q: 31.834627, mean_eps: 0.845582\n",
      "  3471/20000: episode: 138, duration: 0.081s, episode steps:  19, steps per second: 236, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 2.929828, mae: 15.463174, mean_q: 31.640680, mean_eps: 0.844255\n",
      "  3482/20000: episode: 139, duration: 0.048s, episode steps:  11, steps per second: 229, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.693384, mae: 15.576719, mean_q: 31.850933, mean_eps: 0.843580\n",
      "  3515/20000: episode: 140, duration: 0.139s, episode steps:  33, steps per second: 237, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.606 [0.000, 1.000],  loss: 4.295598, mae: 15.627567, mean_q: 31.689109, mean_eps: 0.842590\n",
      "  3531/20000: episode: 141, duration: 0.069s, episode steps:  16, steps per second: 234, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.464407, mae: 15.758030, mean_q: 32.478225, mean_eps: 0.841487\n",
      "  3570/20000: episode: 142, duration: 0.164s, episode steps:  39, steps per second: 238, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 3.595886, mae: 15.824943, mean_q: 32.314724, mean_eps: 0.840250\n",
      "  3602/20000: episode: 143, duration: 0.136s, episode steps:  32, steps per second: 236, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.273060, mae: 15.807843, mean_q: 32.399907, mean_eps: 0.838653\n",
      "  3625/20000: episode: 144, duration: 0.098s, episode steps:  23, steps per second: 234, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 3.625036, mae: 16.242285, mean_q: 32.985709, mean_eps: 0.837415\n",
      "  3638/20000: episode: 145, duration: 0.057s, episode steps:  13, steps per second: 228, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 3.609016, mae: 16.190396, mean_q: 33.047080, mean_eps: 0.836605\n",
      "  3705/20000: episode: 146, duration: 0.276s, episode steps:  67, steps per second: 242, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 3.043284, mae: 16.246266, mean_q: 33.214956, mean_eps: 0.834805\n",
      "  3762/20000: episode: 147, duration: 0.235s, episode steps:  57, steps per second: 243, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 3.464369, mae: 16.579341, mean_q: 34.003315, mean_eps: 0.832015\n",
      "  3798/20000: episode: 148, duration: 0.149s, episode steps:  36, steps per second: 241, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.031968, mae: 16.429849, mean_q: 33.769144, mean_eps: 0.829923\n",
      "  3811/20000: episode: 149, duration: 0.059s, episode steps:  13, steps per second: 220, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 3.139183, mae: 17.332088, mean_q: 35.324718, mean_eps: 0.828820\n",
      "  3834/20000: episode: 150, duration: 0.099s, episode steps:  23, steps per second: 232, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 2.894688, mae: 17.227892, mean_q: 35.446345, mean_eps: 0.828010\n",
      "  3847/20000: episode: 151, duration: 0.057s, episode steps:  13, steps per second: 229, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 4.377510, mae: 17.118574, mean_q: 35.174850, mean_eps: 0.827200\n",
      "  3868/20000: episode: 152, duration: 0.089s, episode steps:  21, steps per second: 237, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.446972, mae: 17.343150, mean_q: 35.455777, mean_eps: 0.826435\n",
      "  3881/20000: episode: 153, duration: 0.057s, episode steps:  13, steps per second: 227, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 3.084326, mae: 17.247618, mean_q: 35.652962, mean_eps: 0.825670\n",
      "  3940/20000: episode: 154, duration: 0.246s, episode steps:  59, steps per second: 240, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 3.366069, mae: 17.377821, mean_q: 35.606360, mean_eps: 0.824050\n",
      "  3961/20000: episode: 155, duration: 0.089s, episode steps:  21, steps per second: 237, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.115158, mae: 17.963721, mean_q: 36.952636, mean_eps: 0.822250\n",
      "  4036/20000: episode: 156, duration: 0.311s, episode steps:  75, steps per second: 241, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.831339, mae: 17.968934, mean_q: 36.936613, mean_eps: 0.820090\n",
      "  4045/20000: episode: 157, duration: 0.047s, episode steps:   9, steps per second: 191, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 3.139491, mae: 18.146418, mean_q: 37.550720, mean_eps: 0.818200\n",
      "  4107/20000: episode: 158, duration: 0.295s, episode steps:  62, steps per second: 210, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.056777, mae: 18.334468, mean_q: 37.565723, mean_eps: 0.816602\n",
      "  4142/20000: episode: 159, duration: 0.158s, episode steps:  35, steps per second: 222, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.024513, mae: 18.903833, mean_q: 38.674806, mean_eps: 0.814420\n",
      "  4166/20000: episode: 160, duration: 0.109s, episode steps:  24, steps per second: 221, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.666387, mae: 18.625461, mean_q: 38.463526, mean_eps: 0.813093\n",
      "  4214/20000: episode: 161, duration: 0.218s, episode steps:  48, steps per second: 221, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.390499, mae: 18.946139, mean_q: 39.009271, mean_eps: 0.811472\n",
      "  4280/20000: episode: 162, duration: 0.304s, episode steps:  66, steps per second: 217, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.909021, mae: 19.419022, mean_q: 39.995415, mean_eps: 0.808908\n",
      "  4331/20000: episode: 163, duration: 0.225s, episode steps:  51, steps per second: 227, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  loss: 4.458275, mae: 19.674669, mean_q: 40.432540, mean_eps: 0.806275\n",
      "  4342/20000: episode: 164, duration: 0.054s, episode steps:  11, steps per second: 203, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 3.826457, mae: 19.966156, mean_q: 41.039401, mean_eps: 0.804880\n",
      "  4374/20000: episode: 165, duration: 0.139s, episode steps:  32, steps per second: 231, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 3.687159, mae: 20.171156, mean_q: 41.419345, mean_eps: 0.803913\n",
      "  4397/20000: episode: 166, duration: 0.102s, episode steps:  23, steps per second: 226, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 4.208002, mae: 19.893068, mean_q: 40.862009, mean_eps: 0.802675\n",
      "  4446/20000: episode: 167, duration: 0.219s, episode steps:  49, steps per second: 224, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 2.994415, mae: 20.463976, mean_q: 42.289644, mean_eps: 0.801055\n",
      "  4539/20000: episode: 168, duration: 0.402s, episode steps:  93, steps per second: 232, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.020722, mae: 20.837209, mean_q: 43.095429, mean_eps: 0.797860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4586/20000: episode: 169, duration: 0.219s, episode steps:  47, steps per second: 214, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.863813, mae: 20.937207, mean_q: 43.148691, mean_eps: 0.794710\n",
      "  4600/20000: episode: 170, duration: 0.067s, episode steps:  14, steps per second: 209, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 6.580069, mae: 21.106380, mean_q: 43.336383, mean_eps: 0.793337\n",
      "  4619/20000: episode: 171, duration: 0.087s, episode steps:  19, steps per second: 219, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.236473, mae: 21.835896, mean_q: 45.003930, mean_eps: 0.792595\n",
      "  4670/20000: episode: 172, duration: 0.224s, episode steps:  51, steps per second: 228, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  loss: 4.816658, mae: 21.623977, mean_q: 44.529898, mean_eps: 0.791020\n",
      "  4733/20000: episode: 173, duration: 0.285s, episode steps:  63, steps per second: 221, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.559797, mae: 21.727754, mean_q: 44.740246, mean_eps: 0.788455\n",
      "  4760/20000: episode: 174, duration: 0.120s, episode steps:  27, steps per second: 225, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.817202, mae: 21.939156, mean_q: 45.320613, mean_eps: 0.786430\n",
      "  4868/20000: episode: 175, duration: 0.465s, episode steps: 108, steps per second: 232, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.088847, mae: 22.441358, mean_q: 46.286301, mean_eps: 0.783393\n",
      "  4885/20000: episode: 176, duration: 0.081s, episode steps:  17, steps per second: 211, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 3.719024, mae: 22.350761, mean_q: 46.557778, mean_eps: 0.780580\n",
      "  4973/20000: episode: 177, duration: 0.373s, episode steps:  88, steps per second: 236, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.915888, mae: 22.989864, mean_q: 47.596356, mean_eps: 0.778217\n",
      "  4987/20000: episode: 178, duration: 0.060s, episode steps:  14, steps per second: 232, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 3.222612, mae: 22.703272, mean_q: 47.592512, mean_eps: 0.775922\n",
      "  5001/20000: episode: 179, duration: 0.062s, episode steps:  14, steps per second: 226, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.250762, mae: 22.411477, mean_q: 47.041425, mean_eps: 0.775293\n",
      "  5051/20000: episode: 180, duration: 0.243s, episode steps:  50, steps per second: 206, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.528681, mae: 23.769686, mean_q: 49.237912, mean_eps: 0.773853\n",
      "  5068/20000: episode: 181, duration: 0.077s, episode steps:  17, steps per second: 221, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.046206, mae: 23.105749, mean_q: 48.264839, mean_eps: 0.772345\n",
      "  5103/20000: episode: 182, duration: 0.166s, episode steps:  35, steps per second: 211, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.618041, mae: 23.253593, mean_q: 48.602958, mean_eps: 0.771175\n",
      "  5172/20000: episode: 183, duration: 0.325s, episode steps:  69, steps per second: 212, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 4.986621, mae: 24.203770, mean_q: 50.182458, mean_eps: 0.768835\n",
      "  5217/20000: episode: 184, duration: 0.201s, episode steps:  45, steps per second: 223, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.422 [0.000, 1.000],  loss: 5.116533, mae: 24.866000, mean_q: 51.391866, mean_eps: 0.766270\n",
      "  5243/20000: episode: 185, duration: 0.118s, episode steps:  26, steps per second: 220, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.370407, mae: 24.476813, mean_q: 51.037909, mean_eps: 0.764672\n",
      "  5260/20000: episode: 186, duration: 0.082s, episode steps:  17, steps per second: 209, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 6.039161, mae: 24.126987, mean_q: 50.477831, mean_eps: 0.763705\n",
      "  5277/20000: episode: 187, duration: 0.083s, episode steps:  17, steps per second: 204, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 6.540717, mae: 24.588757, mean_q: 50.978333, mean_eps: 0.762940\n",
      "  5328/20000: episode: 188, duration: 0.246s, episode steps:  51, steps per second: 207, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.456176, mae: 25.034362, mean_q: 51.722354, mean_eps: 0.761410\n",
      "  5343/20000: episode: 189, duration: 0.069s, episode steps:  15, steps per second: 218, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 5.266431, mae: 25.368690, mean_q: 52.598353, mean_eps: 0.759925\n",
      "  5358/20000: episode: 190, duration: 0.146s, episode steps:  15, steps per second: 103, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.604492, mae: 25.717143, mean_q: 52.912507, mean_eps: 0.759250\n",
      "  5382/20000: episode: 191, duration: 0.133s, episode steps:  24, steps per second: 181, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 4.503000, mae: 25.081975, mean_q: 52.083884, mean_eps: 0.758373\n",
      "  5500/20000: episode: 192, duration: 0.526s, episode steps: 118, steps per second: 224, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.622913, mae: 25.586757, mean_q: 53.020280, mean_eps: 0.755177\n",
      "  5519/20000: episode: 193, duration: 0.084s, episode steps:  19, steps per second: 227, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 4.860546, mae: 26.271730, mean_q: 54.369240, mean_eps: 0.752095\n",
      "  5552/20000: episode: 194, duration: 0.138s, episode steps:  33, steps per second: 239, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.908686, mae: 26.394735, mean_q: 54.536672, mean_eps: 0.750925\n",
      "  5607/20000: episode: 195, duration: 0.235s, episode steps:  55, steps per second: 234, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.969258, mae: 26.604348, mean_q: 54.854736, mean_eps: 0.748945\n",
      "  5632/20000: episode: 196, duration: 0.107s, episode steps:  25, steps per second: 233, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 5.445266, mae: 27.373899, mean_q: 56.714393, mean_eps: 0.747145\n",
      "  5689/20000: episode: 197, duration: 0.235s, episode steps:  57, steps per second: 243, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 5.943184, mae: 27.250377, mean_q: 56.340146, mean_eps: 0.745300\n",
      "  5755/20000: episode: 198, duration: 0.274s, episode steps:  66, steps per second: 241, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.755680, mae: 27.726234, mean_q: 57.303179, mean_eps: 0.742533\n",
      "  5792/20000: episode: 199, duration: 0.156s, episode steps:  37, steps per second: 238, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.648258, mae: 27.743396, mean_q: 57.496311, mean_eps: 0.740215\n",
      "  5808/20000: episode: 200, duration: 0.072s, episode steps:  16, steps per second: 222, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.351954, mae: 27.676317, mean_q: 56.908871, mean_eps: 0.739022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5944/20000: episode: 201, duration: 0.569s, episode steps: 136, steps per second: 239, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.262288, mae: 28.467505, mean_q: 59.076422, mean_eps: 0.735602\n",
      "  6059/20000: episode: 202, duration: 0.473s, episode steps: 115, steps per second: 243, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 5.801315, mae: 29.325321, mean_q: 60.743709, mean_eps: 0.729955\n",
      "  6127/20000: episode: 203, duration: 0.286s, episode steps:  68, steps per second: 238, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.574 [0.000, 1.000],  loss: 8.887407, mae: 29.626598, mean_q: 60.991265, mean_eps: 0.725838\n",
      "  6173/20000: episode: 204, duration: 0.190s, episode steps:  46, steps per second: 243, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.022113, mae: 29.752230, mean_q: 61.480047, mean_eps: 0.723272\n",
      "  6188/20000: episode: 205, duration: 0.066s, episode steps:  15, steps per second: 227, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.118692, mae: 30.038025, mean_q: 62.238096, mean_eps: 0.721900\n",
      "  6258/20000: episode: 206, duration: 0.312s, episode steps:  70, steps per second: 224, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.801945, mae: 30.507185, mean_q: 63.018800, mean_eps: 0.719988\n",
      "  6279/20000: episode: 207, duration: 0.091s, episode steps:  21, steps per second: 232, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.712529, mae: 30.234668, mean_q: 63.023417, mean_eps: 0.717940\n",
      "  6297/20000: episode: 208, duration: 0.077s, episode steps:  18, steps per second: 235, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.698594, mae: 30.196123, mean_q: 62.527505, mean_eps: 0.717063\n",
      "  6348/20000: episode: 209, duration: 0.214s, episode steps:  51, steps per second: 238, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.833104, mae: 31.631541, mean_q: 65.223591, mean_eps: 0.715510\n",
      "  6371/20000: episode: 210, duration: 0.097s, episode steps:  23, steps per second: 236, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 6.119912, mae: 31.932077, mean_q: 66.164537, mean_eps: 0.713845\n",
      "  6532/20000: episode: 211, duration: 0.691s, episode steps: 161, steps per second: 233, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 8.330110, mae: 32.238378, mean_q: 66.722208, mean_eps: 0.709705\n",
      "  6578/20000: episode: 212, duration: 0.192s, episode steps:  46, steps per second: 240, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.728970, mae: 33.096319, mean_q: 68.566018, mean_eps: 0.705048\n",
      "  6626/20000: episode: 213, duration: 0.201s, episode steps:  48, steps per second: 239, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 9.937113, mae: 33.408455, mean_q: 68.902456, mean_eps: 0.702933\n",
      "  6702/20000: episode: 214, duration: 0.315s, episode steps:  76, steps per second: 242, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 9.680799, mae: 33.397299, mean_q: 69.144819, mean_eps: 0.700143\n",
      "  6713/20000: episode: 215, duration: 0.048s, episode steps:  11, steps per second: 227, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 7.090446, mae: 34.071059, mean_q: 70.958428, mean_eps: 0.698185\n",
      "  6750/20000: episode: 216, duration: 0.153s, episode steps:  37, steps per second: 242, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 10.056620, mae: 34.297764, mean_q: 70.672079, mean_eps: 0.697105\n",
      "  6883/20000: episode: 217, duration: 0.560s, episode steps: 133, steps per second: 238, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 9.270520, mae: 34.481701, mean_q: 71.352442, mean_eps: 0.693280\n",
      "  6901/20000: episode: 218, duration: 0.083s, episode steps:  18, steps per second: 216, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.854907, mae: 33.916951, mean_q: 70.261414, mean_eps: 0.689883\n",
      "  6987/20000: episode: 219, duration: 0.360s, episode steps:  86, steps per second: 239, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 8.491849, mae: 35.540751, mean_q: 73.348756, mean_eps: 0.687543\n",
      "  7005/20000: episode: 220, duration: 0.079s, episode steps:  18, steps per second: 227, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 4.809309, mae: 36.211466, mean_q: 75.072915, mean_eps: 0.685203\n",
      "  7032/20000: episode: 221, duration: 0.114s, episode steps:  27, steps per second: 238, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 11.695241, mae: 36.213593, mean_q: 74.680030, mean_eps: 0.684190\n",
      "  7064/20000: episode: 222, duration: 0.147s, episode steps:  32, steps per second: 218, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 9.619217, mae: 37.177755, mean_q: 76.431878, mean_eps: 0.682862\n",
      "  7084/20000: episode: 223, duration: 0.097s, episode steps:  20, steps per second: 207, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.345675, mae: 36.354974, mean_q: 74.954963, mean_eps: 0.681692\n",
      "  7123/20000: episode: 224, duration: 0.204s, episode steps:  39, steps per second: 191, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 6.587623, mae: 36.799567, mean_q: 76.267577, mean_eps: 0.680365\n",
      "  7160/20000: episode: 225, duration: 0.190s, episode steps:  37, steps per second: 195, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 9.083571, mae: 36.962940, mean_q: 76.197141, mean_eps: 0.678655\n",
      "  7195/20000: episode: 226, duration: 0.206s, episode steps:  35, steps per second: 170, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 11.559422, mae: 37.300193, mean_q: 77.269036, mean_eps: 0.677035\n",
      "  7252/20000: episode: 227, duration: 0.302s, episode steps:  57, steps per second: 189, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 12.788768, mae: 37.860594, mean_q: 77.994496, mean_eps: 0.674965\n",
      "  7415/20000: episode: 228, duration: 0.767s, episode steps: 163, steps per second: 213, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 9.669928, mae: 38.123149, mean_q: 78.747442, mean_eps: 0.670015\n",
      "  7530/20000: episode: 229, duration: 0.498s, episode steps: 115, steps per second: 231, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 11.347096, mae: 38.653028, mean_q: 79.750308, mean_eps: 0.663760\n",
      "  7575/20000: episode: 230, duration: 0.204s, episode steps:  45, steps per second: 221, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 10.897511, mae: 39.697823, mean_q: 81.789557, mean_eps: 0.660160\n",
      "  7604/20000: episode: 231, duration: 0.142s, episode steps:  29, steps per second: 205, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 12.952499, mae: 39.570967, mean_q: 81.835094, mean_eps: 0.658495\n",
      "  7706/20000: episode: 232, duration: 0.441s, episode steps: 102, steps per second: 231, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 12.987162, mae: 39.667289, mean_q: 81.917984, mean_eps: 0.655547\n",
      "  7747/20000: episode: 233, duration: 0.181s, episode steps:  41, steps per second: 227, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.415 [0.000, 1.000],  loss: 7.689909, mae: 40.760665, mean_q: 84.301171, mean_eps: 0.652330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7794/20000: episode: 234, duration: 0.210s, episode steps:  47, steps per second: 224, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 12.341644, mae: 40.895205, mean_q: 84.590269, mean_eps: 0.650350\n",
      "  7929/20000: episode: 235, duration: 0.634s, episode steps: 135, steps per second: 213, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 13.718714, mae: 41.138760, mean_q: 85.119819, mean_eps: 0.646255\n",
      "  8108/20000: episode: 236, duration: 0.741s, episode steps: 179, steps per second: 241, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 15.427792, mae: 41.569186, mean_q: 85.826531, mean_eps: 0.639190\n",
      "  8144/20000: episode: 237, duration: 0.150s, episode steps:  36, steps per second: 240, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.953295, mae: 42.255388, mean_q: 87.620568, mean_eps: 0.634353\n",
      "  8171/20000: episode: 238, duration: 0.113s, episode steps:  27, steps per second: 239, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 6.958788, mae: 41.578844, mean_q: 86.771395, mean_eps: 0.632935\n",
      "  8276/20000: episode: 239, duration: 0.450s, episode steps: 105, steps per second: 233, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 13.634241, mae: 42.585813, mean_q: 88.074526, mean_eps: 0.629965\n",
      "  8301/20000: episode: 240, duration: 0.117s, episode steps:  25, steps per second: 214, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 17.102110, mae: 42.539330, mean_q: 87.980733, mean_eps: 0.627040\n",
      "  8342/20000: episode: 241, duration: 0.179s, episode steps:  41, steps per second: 229, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 12.780089, mae: 44.120653, mean_q: 90.907013, mean_eps: 0.625555\n",
      "  8408/20000: episode: 242, duration: 0.276s, episode steps:  66, steps per second: 239, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 14.872430, mae: 44.090860, mean_q: 91.057571, mean_eps: 0.623147\n",
      "  8571/20000: episode: 243, duration: 0.700s, episode steps: 163, steps per second: 233, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 14.947470, mae: 45.079323, mean_q: 93.085769, mean_eps: 0.617995\n",
      "  8613/20000: episode: 244, duration: 0.180s, episode steps:  42, steps per second: 234, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 13.781965, mae: 44.679794, mean_q: 92.749425, mean_eps: 0.613382\n",
      "  8626/20000: episode: 245, duration: 0.060s, episode steps:  13, steps per second: 216, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 14.404013, mae: 45.997850, mean_q: 95.477111, mean_eps: 0.612145\n",
      "  8760/20000: episode: 246, duration: 0.580s, episode steps: 134, steps per second: 231, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 13.348398, mae: 46.283600, mean_q: 95.585657, mean_eps: 0.608838\n",
      "  8928/20000: episode: 247, duration: 0.869s, episode steps: 168, steps per second: 193, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.669092, mae: 47.145341, mean_q: 97.475267, mean_eps: 0.602042\n",
      "  8971/20000: episode: 248, duration: 0.299s, episode steps:  43, steps per second: 144, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 17.659411, mae: 47.660419, mean_q: 98.571321, mean_eps: 0.597295\n",
      "  9131/20000: episode: 249, duration: 0.834s, episode steps: 160, steps per second: 192, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 15.865134, mae: 48.522375, mean_q: 100.355471, mean_eps: 0.592727\n",
      "  9155/20000: episode: 250, duration: 0.104s, episode steps:  24, steps per second: 231, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15.574451, mae: 48.742965, mean_q: 100.670251, mean_eps: 0.588587\n",
      "  9324/20000: episode: 251, duration: 0.745s, episode steps: 169, steps per second: 227, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 16.340798, mae: 50.237235, mean_q: 103.649794, mean_eps: 0.584245\n",
      "  9516/20000: episode: 252, duration: 0.799s, episode steps: 192, steps per second: 240, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 18.201146, mae: 51.569345, mean_q: 106.430573, mean_eps: 0.576122\n",
      "  9716/20000: episode: 253, duration: 0.822s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 20.765669, mae: 52.909159, mean_q: 109.062984, mean_eps: 0.567303\n",
      "  9735/20000: episode: 254, duration: 0.082s, episode steps:  19, steps per second: 232, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 10.927743, mae: 54.262753, mean_q: 111.550370, mean_eps: 0.562375\n",
      "  9775/20000: episode: 255, duration: 0.166s, episode steps:  40, steps per second: 241, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 10.969690, mae: 54.514361, mean_q: 112.523022, mean_eps: 0.561047\n",
      "  9837/20000: episode: 256, duration: 0.263s, episode steps:  62, steps per second: 235, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 17.940719, mae: 54.057933, mean_q: 111.355939, mean_eps: 0.558753\n",
      "  9860/20000: episode: 257, duration: 0.099s, episode steps:  23, steps per second: 233, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 22.064691, mae: 54.400638, mean_q: 112.456290, mean_eps: 0.556840\n",
      "  9930/20000: episode: 258, duration: 0.292s, episode steps:  70, steps per second: 240, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 14.833811, mae: 54.817777, mean_q: 113.114455, mean_eps: 0.554748\n",
      " 10011/20000: episode: 259, duration: 0.337s, episode steps:  81, steps per second: 240, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 15.929421, mae: 54.932383, mean_q: 113.792008, mean_eps: 0.551350\n",
      " 10094/20000: episode: 260, duration: 0.342s, episode steps:  83, steps per second: 243, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 25.336245, mae: 55.644473, mean_q: 114.549728, mean_eps: 0.547660\n",
      " 10126/20000: episode: 261, duration: 0.136s, episode steps:  32, steps per second: 235, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 25.419298, mae: 55.834209, mean_q: 114.671621, mean_eps: 0.545073\n",
      " 10223/20000: episode: 262, duration: 0.404s, episode steps:  97, steps per second: 240, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 20.576466, mae: 55.637734, mean_q: 114.839554, mean_eps: 0.542170\n",
      " 10325/20000: episode: 263, duration: 0.436s, episode steps: 102, steps per second: 234, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 25.397354, mae: 56.485188, mean_q: 116.540519, mean_eps: 0.537693\n",
      " 10513/20000: episode: 264, duration: 0.801s, episode steps: 188, steps per second: 235, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 18.989875, mae: 56.931477, mean_q: 117.512720, mean_eps: 0.531168\n",
      " 10664/20000: episode: 265, duration: 0.622s, episode steps: 151, steps per second: 243, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 23.822827, mae: 58.789059, mean_q: 121.059163, mean_eps: 0.523540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10851/20000: episode: 266, duration: 0.777s, episode steps: 187, steps per second: 241, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 27.163293, mae: 59.549428, mean_q: 122.613416, mean_eps: 0.515935\n",
      " 10878/20000: episode: 267, duration: 0.117s, episode steps:  27, steps per second: 232, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 21.583591, mae: 59.846627, mean_q: 123.380839, mean_eps: 0.511120\n",
      " 11078/20000: episode: 268, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 23.161512, mae: 61.023298, mean_q: 125.686100, mean_eps: 0.506012\n",
      " 11278/20000: episode: 269, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 23.619820, mae: 62.520616, mean_q: 128.805550, mean_eps: 0.497012\n",
      " 11409/20000: episode: 270, duration: 0.556s, episode steps: 131, steps per second: 236, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 29.801277, mae: 62.695672, mean_q: 128.817488, mean_eps: 0.489565\n",
      " 11489/20000: episode: 271, duration: 0.332s, episode steps:  80, steps per second: 241, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.575 [0.000, 1.000],  loss: 18.936939, mae: 63.481519, mean_q: 130.653242, mean_eps: 0.484817\n",
      " 11623/20000: episode: 272, duration: 0.555s, episode steps: 134, steps per second: 242, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 19.268421, mae: 64.716874, mean_q: 133.334807, mean_eps: 0.480002\n",
      " 11687/20000: episode: 273, duration: 0.264s, episode steps:  64, steps per second: 242, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 25.446371, mae: 65.049718, mean_q: 133.986654, mean_eps: 0.475547\n",
      " 11887/20000: episode: 274, duration: 0.824s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 28.503263, mae: 65.035719, mean_q: 133.729639, mean_eps: 0.469607\n",
      " 12087/20000: episode: 275, duration: 0.822s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 30.065359, mae: 67.039615, mean_q: 138.049979, mean_eps: 0.460608\n",
      " 12136/20000: episode: 276, duration: 0.205s, episode steps:  49, steps per second: 239, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 25.628993, mae: 66.859930, mean_q: 137.920250, mean_eps: 0.455005\n",
      " 12288/20000: episode: 277, duration: 0.626s, episode steps: 152, steps per second: 243, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 28.277868, mae: 68.811989, mean_q: 141.538671, mean_eps: 0.450482\n",
      " 12367/20000: episode: 278, duration: 0.328s, episode steps:  79, steps per second: 241, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 31.617062, mae: 69.693325, mean_q: 142.922031, mean_eps: 0.445285\n",
      " 12567/20000: episode: 279, duration: 0.824s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 29.399835, mae: 69.955420, mean_q: 143.826522, mean_eps: 0.439007\n",
      " 12767/20000: episode: 280, duration: 0.822s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 28.507441, mae: 71.013593, mean_q: 146.087213, mean_eps: 0.430007\n",
      " 12967/20000: episode: 281, duration: 0.824s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 28.791188, mae: 72.245004, mean_q: 148.906612, mean_eps: 0.421007\n",
      " 13166/20000: episode: 282, duration: 0.821s, episode steps: 199, steps per second: 242, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 32.991970, mae: 72.618521, mean_q: 149.337602, mean_eps: 0.412030\n",
      " 13366/20000: episode: 283, duration: 0.823s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 31.642086, mae: 73.851951, mean_q: 151.361053, mean_eps: 0.403053\n",
      " 13467/20000: episode: 284, duration: 0.419s, episode steps: 101, steps per second: 241, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 30.745224, mae: 73.956324, mean_q: 151.934626, mean_eps: 0.396280\n",
      " 13660/20000: episode: 285, duration: 0.795s, episode steps: 193, steps per second: 243, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 34.909303, mae: 73.681842, mean_q: 151.219959, mean_eps: 0.389665\n",
      " 13693/20000: episode: 286, duration: 0.152s, episode steps:  33, steps per second: 217, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 35.318740, mae: 73.632694, mean_q: 150.326610, mean_eps: 0.384580\n",
      " 13893/20000: episode: 287, duration: 0.858s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 32.667721, mae: 75.023644, mean_q: 153.857571, mean_eps: 0.379337\n",
      " 14053/20000: episode: 288, duration: 0.845s, episode steps: 160, steps per second: 189, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 33.656189, mae: 76.740013, mean_q: 157.422235, mean_eps: 0.371237\n",
      " 14253/20000: episode: 289, duration: 0.996s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 35.746322, mae: 77.472888, mean_q: 158.760690, mean_eps: 0.363138\n",
      " 14282/20000: episode: 290, duration: 0.122s, episode steps:  29, steps per second: 238, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 53.472512, mae: 78.529047, mean_q: 160.802894, mean_eps: 0.357985\n",
      " 14373/20000: episode: 291, duration: 0.378s, episode steps:  91, steps per second: 240, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 44.399743, mae: 78.638141, mean_q: 160.893235, mean_eps: 0.355285\n",
      " 14539/20000: episode: 292, duration: 0.688s, episode steps: 166, steps per second: 241, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 32.020998, mae: 78.170505, mean_q: 160.338116, mean_eps: 0.349502\n",
      " 14721/20000: episode: 293, duration: 0.750s, episode steps: 182, steps per second: 243, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 34.088695, mae: 78.101234, mean_q: 159.890383, mean_eps: 0.341672\n",
      " 14921/20000: episode: 294, duration: 0.832s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 34.889366, mae: 77.704447, mean_q: 159.461090, mean_eps: 0.333077\n",
      " 15037/20000: episode: 295, duration: 0.502s, episode steps: 116, steps per second: 231, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 27.307250, mae: 77.939548, mean_q: 159.866457, mean_eps: 0.325967\n",
      " 15237/20000: episode: 296, duration: 0.825s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 38.587745, mae: 78.026185, mean_q: 159.756450, mean_eps: 0.318857\n",
      " 15345/20000: episode: 297, duration: 0.449s, episode steps: 108, steps per second: 240, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 30.404163, mae: 79.570526, mean_q: 163.123295, mean_eps: 0.311927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15545/20000: episode: 298, duration: 0.833s, episode steps: 200, steps per second: 240, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 41.435319, mae: 79.867498, mean_q: 163.427245, mean_eps: 0.304997\n",
      " 15745/20000: episode: 299, duration: 0.825s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 36.292743, mae: 80.527873, mean_q: 164.854713, mean_eps: 0.295997\n",
      " 15945/20000: episode: 300, duration: 0.823s, episode steps: 200, steps per second: 243, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 40.517832, mae: 81.366424, mean_q: 166.327461, mean_eps: 0.286997\n",
      " 16145/20000: episode: 301, duration: 0.827s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 31.052033, mae: 81.082701, mean_q: 166.059727, mean_eps: 0.277997\n",
      " 16345/20000: episode: 302, duration: 0.859s, episode steps: 200, steps per second: 233, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 31.122092, mae: 81.759072, mean_q: 166.768320, mean_eps: 0.268997\n",
      " 16545/20000: episode: 303, duration: 0.826s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 42.325683, mae: 81.176477, mean_q: 165.721558, mean_eps: 0.259997\n",
      " 16745/20000: episode: 304, duration: 0.828s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 34.629182, mae: 81.073296, mean_q: 165.340525, mean_eps: 0.250997\n",
      " 16945/20000: episode: 305, duration: 0.829s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 31.041735, mae: 81.153072, mean_q: 165.516581, mean_eps: 0.241997\n",
      " 17145/20000: episode: 306, duration: 0.825s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 34.236350, mae: 80.757761, mean_q: 165.234538, mean_eps: 0.232997\n",
      " 17345/20000: episode: 307, duration: 0.827s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 31.615582, mae: 81.289481, mean_q: 166.140347, mean_eps: 0.223997\n",
      " 17545/20000: episode: 308, duration: 0.831s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 36.218601, mae: 81.890341, mean_q: 166.918673, mean_eps: 0.214997\n",
      " 17745/20000: episode: 309, duration: 0.826s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 29.847680, mae: 81.152735, mean_q: 165.305459, mean_eps: 0.205997\n",
      " 17945/20000: episode: 310, duration: 0.828s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 33.355267, mae: 81.549650, mean_q: 166.012151, mean_eps: 0.196997\n",
      " 18145/20000: episode: 311, duration: 0.829s, episode steps: 200, steps per second: 241, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 41.257691, mae: 81.983144, mean_q: 167.190430, mean_eps: 0.187997\n",
      " 18345/20000: episode: 312, duration: 0.828s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 36.000986, mae: 82.284356, mean_q: 168.028802, mean_eps: 0.178997\n",
      " 18545/20000: episode: 313, duration: 0.862s, episode steps: 200, steps per second: 232, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 28.553449, mae: 81.674547, mean_q: 166.364597, mean_eps: 0.169997\n",
      " 18745/20000: episode: 314, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 27.006238, mae: 80.743845, mean_q: 164.647024, mean_eps: 0.160997\n",
      " 18945/20000: episode: 315, duration: 0.864s, episode steps: 200, steps per second: 231, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 55.522379, mae: 80.147136, mean_q: 162.669985, mean_eps: 0.151997\n",
      " 19145/20000: episode: 316, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 30.675206, mae: 80.310982, mean_q: 163.855535, mean_eps: 0.142997\n",
      " 19345/20000: episode: 317, duration: 1.032s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 30.934104, mae: 80.735347, mean_q: 164.408366, mean_eps: 0.133997\n",
      " 19545/20000: episode: 318, duration: 0.903s, episode steps: 200, steps per second: 221, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 37.629222, mae: 81.772508, mean_q: 166.710157, mean_eps: 0.124997\n",
      " 19745/20000: episode: 319, duration: 0.845s, episode steps: 200, steps per second: 237, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 31.431815, mae: 81.057046, mean_q: 165.217656, mean_eps: 0.115997\n",
      " 19945/20000: episode: 320, duration: 0.850s, episode steps: 200, steps per second: 235, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.144066, mae: 82.589027, mean_q: 168.045817, mean_eps: 0.106997\n",
      "done, took 86.790 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb46539208>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b11454ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(f'my_weights_cartpole.h5f', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b76eb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn.load_weights('my_weights_cartpole.h5f.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "371592da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18748\\1345061794.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[0;32m    350\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[1;34m(self, action, logs)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'on_action_end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[1;34m(self, action, logs)\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_action_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;34m\"\"\" Render environment at the end of each action \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;34m\"\"\"Renders the environment with kwargs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\wrappers\\env_checker.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpassive_env_render_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bff6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef929110",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbaea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
